{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Tokenization and Cleaning with NLTK\n",
    "@ Sani Kamal, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit, or NLTK for short, is a Python library written for working and modeling text. It provides good tools for loading and cleaning text that we can use to get our data ready for working with machine learning and deep learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install NLTK\n",
    "You can install NLTK using your favorite package manager, such as pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "# Or from the command line\n",
    "# python -m nltk.downloader all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'data/fireless_cook_book_clean.txt'\n",
    "file = open(filename,'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Sentences\n",
    "NLTK provides the `sent_tokenize()` function to split text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roast meats, however, may first be\n",
      "cooked in the oven and completed in the hay-box or cooker, or they may\n",
      "be cooked in the hay-box till nearly done and then roasted for a short\n",
      "time to obtain the crispness which can be given only by cooking with\n",
      "great heat.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "# split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Words\n",
    "NLTK provides a function called `word_tokenize()` for splitting strings into tokens (nominally words). It splits tokens based on white space and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'FIRELESS', 'COOKER', 'Does', 'the', 'idea', 'appeal', 'to', 'you', 'of', 'putting', 'your', 'dinner', 'on', 'to', 'cook', 'and', 'then', 'going', 'visiting', ',', 'or', 'to', 'the', 'theatre', ',', 'or', 'sitting', 'down', 'to', 'read', ',', 'write', ',', 'or', 'sew', ',', 'with', 'no', 'further', 'thought', 'for', 'your', 'food', 'until', 'it', 'is', 'time', 'to', 'serve', 'it', '?', 'It', 'sounds', 'like', 'a', 'fairy-tale', 'to', 'say', 'that', 'you', 'can', 'bring', 'food', 'to', 'the', 'boiling', 'point', ',', 'put', 'it', 'into', 'a', 'box', 'of', 'hay', ',', 'and', 'leave', 'it', 'for', 'a', 'few', 'hours', ',', 'returning', 'to', 'find', 'it', 'cooked', ',', 'and', 'often', 'better', 'cooked', 'than', 'in', 'any', 'other', 'way']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Out Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THE', 'FIRELESS', 'COOKER', 'Does', 'the', 'idea', 'appeal', 'to', 'you', 'of', 'putting', 'your', 'dinner', 'on', 'to', 'cook', 'and', 'then', 'going', 'visiting', 'or', 'to', 'the', 'theatre', 'or', 'sitting', 'down', 'to', 'read', 'write', 'or', 'sew', 'with', 'no', 'further', 'thought', 'for', 'your', 'food', 'until', 'it', 'is', 'time', 'to', 'serve', 'it', 'It', 'sounds', 'like', 'a', 'to', 'say', 'that', 'you', 'can', 'bring', 'food', 'to', 'the', 'boiling', 'point', 'put', 'it', 'into', 'a', 'box', 'of', 'hay', 'and', 'leave', 'it', 'for', 'a', 'few', 'hours', 'returning', 'to', 'find', 'it', 'cooked', 'and', 'often', 'better', 'cooked', 'than', 'in', 'any', 'other', 'way', 'Yet', 'it', 'is', 'true', 'Norwegian', 'housewives', 'have', 'known', 'this', 'for', 'many']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Stop Words\n",
    "`Stop words` are those words that do not contribute to the deeper meaning of the phrase. They are the most common words such as: `the`, `a`, and `is`. For some applications like documentation classification, it may make sense to remove stop words. `NLTK` provides a list of commonly agreed upon stop words for a variety of languages, such as English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple pipeline of text preparation\n",
    "- Load the raw text.\n",
    "- Split into tokens.\n",
    "- Convert to lowercase.\n",
    "- Remove punctuation from each token.\n",
    "- Filter out remaining tokens that are not alphabetic.\n",
    "- Filter out tokens that are stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fireless', 'cooker', 'idea', 'appeal', 'putting', 'dinner', 'cook', 'going', 'visiting', 'theatre', 'sitting', 'read', 'write', 'sew', 'thought', 'food', 'time', 'serve', 'sounds', 'like', 'fairytale', 'say', 'bring', 'food', 'boiling', 'point', 'put', 'box', 'hay', 'leave', 'hours', 'returning', 'find', 'cooked', 'often', 'better', 'cooked', 'way', 'yet', 'true', 'norwegian', 'housewives', 'known', 'many', 'years', 'european', 'nations', 'used', 'haybox', 'considerable', 'extent', 'although', 'recently', 'wonders', 'become', 'rather', 'widely', 'known', 'talked', 'america', 'original', 'box', 'filled', 'hay', 'gone', 'process', 'evolution', 'become', 'fireless', 'cooker', 'varied', 'form', 'adaptability', 'expect', 'fireless', 'cooker', 'foods', 'cook', 'advantage', 'almost', 'dishes', 'usually', 'prepared', 'boiling', 'steaming', 'well', 'many', 'baked', 'soups', 'boiled', 'braised', 'meats', 'fish', 'sauces', 'fruits', 'vegetables', 'puddings', 'eggs', 'fact', 'almost']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# load data\n",
    "filename = 'data/fireless_cook_book_clean.txt'\n",
    "file = open(filename, 'rt' )\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile( '[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub( '' , w) for w in tokens]\n",
    "\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words( 'english' ))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem Words\n",
    "Stemming refers to the process of reducing each word to its root or base. Some applications, like document classification, may benefit from stemming in order to both reduce the vocabulary and to focus on the sense or sentiment of a document rather than deeper meaning. There are many stemming algorithms, although a popular and long-standing method is the Porter Stemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fireless', 'cooker', 'doe', 'the', 'idea', 'appeal', 'to', 'you', 'of', 'put', 'your', 'dinner', 'on', 'to', 'cook', 'and', 'then', 'go', 'visit', ',', 'or', 'to', 'the', 'theatr', ',', 'or', 'sit', 'down', 'to', 'read', ',', 'write', ',', 'or', 'sew', ',', 'with', 'no', 'further', 'thought', 'for', 'your', 'food', 'until', 'it', 'is', 'time', 'to', 'serv', 'it', '?', 'It', 'sound', 'like', 'a', 'fairy-tal', 'to', 'say', 'that', 'you', 'can', 'bring', 'food', 'to', 'the', 'boil', 'point', ',', 'put', 'it', 'into', 'a', 'box', 'of', 'hay', ',', 'and', 'leav', 'it', 'for', 'a', 'few', 'hour', ',', 'return', 'to', 'find', 'it', 'cook', ',', 'and', 'often', 'better', 'cook', 'than', 'in', 'ani', 'other', 'way']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
