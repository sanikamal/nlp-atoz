# NLP AtoZ 📚🚀

Welcome to NLP A to Z, a comprehensive collection of best practices, examples, and mini-projects in Natural Language Processing (NLP). This repo is designed to provide you with a wide range of resources and practical implementations to help you learn and explore the fascinating field of NLP.


## Features 🚀🌟
- 📌Provide a curated collection of best practices and techniques in NLP.
- 📌Showcase practical examples and notebooks for various NLP tasks.
- 📌Mini-projects demonstrating practical NLP applications
- 📌Pre-trained models and datasets for quick experimentation


## Projects and Notebooks 🌐💡

| **Title**     |**Description**                  | **Technology/Library**     | **Link**              |
|---------------|---------------------------------|----------------------------|-----------------------|
| **Clean Text With Python** | Demonstrates various techniques to clean text data, such as removing special characters, handling stopwords, and normalizing text. | `Python` | [Notebook](notebook/clean_text_with_python.ipynb) |
| **Text Tokenization and Cleaning with NLTK** | Focuses on text tokenization and cleaning using the Natural Language Toolkit (NLTK). Includes word tokenization, sentence tokenization, stemming, and lemmatization. | `NLTK` | [Notebook](notebook/cleaning_with_nltk.ipynb) |
| **Prepare Text Data with scikit-learn**  | Showcases text preparation for machine learning tasks using scikit-learn. Covers vectorization, feature extraction, and handling sparse data. | `scikit-learn`    | [Notebook](notebook/prepare_text_data_with_scikit-learn.ipynb) |
| **Prepare Text Data With tf.keras**      | Demonstrates text preparation for deep learning tasks using the tf.keras API. Includes tokenization, padding, and creating word embeddings for sentiment analysis. | `tf.keras` | [Notebook](notebook/prepare_text_with_tf.keras.ipynb) |
| **Comparing Trained LLM Tokenizers** | Exploring different tokenizers associated with various LLMs and analyzing how each tokenizer handles tokenization. | `LLMs`, `Tokenization` | [Notebook](notebook/comparing_llm_tokenizers.ipynb) |
| **Understanding Decoder-Only Transformers with Phi-3** | Exploring the transformer architecture by analyzing the decoder-only model [`microsoft/Phi-3-mini-4k-instruct`](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct). | `Transformers`, `LLMs`, `Microsoft Phi-3` | [Notebook](notebook/understanding_phi3.ipynb) |


Feel free to explore these projects and notebooks to gain practical knowledge and insights into various NLP tasks and techniques.

## Resources 📚📝
For a comprehensive list of additional resources, tutorials, and research papers related to NLP, please refer to the Resources file.

## Contributing 👥🤝
Contributions are highly encouraged! If you have an idea for a new best practice, an interesting example, or a mini-project that you'd like to add, please feel free to submit a pull request. We also welcome bug reports, feature requests, and feedback on existing content. Let's collaborate and make NLP A to Z an even better resource for the community.

## License 📝📜
NLP AtoZ is licensed under the MIT License. See the [LICENSE](LICENSE) file for more details.



